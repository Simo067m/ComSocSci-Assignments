{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02467 Computational Social Science\n",
    "## Assignment 1\n",
    "### Group 15\n",
    "\n",
    "Our GitHub repo is availabe at: https://github.com/Simo067m/ComSocSci-Assignments <br>\n",
    "Contribution:\n",
    "- s233304 : xxxx\n",
    "- s214592 : xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import netwulf\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web-scraping\n",
    "Web-scraping the list of participants to the International Conference in Computational Social Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for finding all unique researchers\n",
    "def scrape_IC2S2(soup : BeautifulSoup):\n",
    "    # Find all the names from the top table\n",
    "    names = []\n",
    "    # Find all the table rows\n",
    "    table_rows = soup.find_all(\"tr\")\n",
    "    for tr in table_rows:\n",
    "        tds = tr.find_all(\"td\")\n",
    "        for row in tds:\n",
    "            a = row.find_all(\"a\")\n",
    "            for text in a:\n",
    "                text_content = text.text\n",
    "                if (\"Keynote\" in text_content):\n",
    "                    text_split = text_content.split(\"-\")\n",
    "                    stripped = text_split[1].strip()\n",
    "                    if (stripped not in names):\n",
    "                        names.append(stripped)\n",
    "    \n",
    "    # Find all the names from the bottom lists\n",
    "    # Find all the unordered lists\n",
    "    ul = soup.find_all(\"ul\", class_=\"nav_list\")\n",
    "    # Find all the list elements\n",
    "    for list in ul:\n",
    "        found_names = list.find_all(\"i\")\n",
    "        # For every found name line, seperate into individual names\n",
    "        for name in found_names:\n",
    "            found_names_seperated = name.text.split(\", \")\n",
    "            for seperated_name in found_names_seperated:\n",
    "                if (seperated_name.strip() not in names):\n",
    "                    names.append(seperated_name.strip())\n",
    "\n",
    "    # Find all the names of the chairs\n",
    "    headers = soup.find_all(\"h2\")\n",
    "    for header in headers:\n",
    "        text = header.find(\"i\")\n",
    "        if (text is not None):\n",
    "            seperated_name = text.text.split(\": \")\n",
    "            if (seperated_name[1].strip() not in names):\n",
    "                names.append(seperated_name[1].strip())\n",
    "\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define url and collect content\n",
    "LINK = \"https://ic2s2-2023.org/program\"\n",
    "r = requests.get(LINK)\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "# find participant names\n",
    "IC2S2_names = scrape_IC2S2(soup)\n",
    "# Save to a pandas DataFrame\n",
    "IC2S2_names_df = pd.DataFrame(IC2S2_names, columns=[\"name\"])\n",
    "IC2S2_names_df.to_csv(\"IC2S2_names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: How many unique researchers do you get?\n",
    ">- 1491"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1491\n"
     ]
    }
   ],
   "source": [
    "print(len(pd.read_csv(\"IC2S2_names.csv\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices **(answer in max 150 words)**.\n",
    "\n",
    ">- By inspecting the webpage, we were able to figure out that names were always contained in an &lt;a&gt; element for displaying the name properly. This means that finding an &lt;a&gt; element within one of the tables containing schedules would guarantee a name. When finding other names, like the ones that have the \"chair\", correctness was ensured by splitting that part from the name, ensuring only the name is retrieved.\n",
    ">- Before adding a new name to the list, there is a check making sure that the name is not already in the found names list before adding it, making sure only unique names are in the list. The names contain no unwanted whitespace by calling the $\\texttt{str.strip()}$ method before adding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ready Made vs Custom Made Data\n",
    "\n",
    "### Q1: What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book **(answer in max 150 words)**.\n",
    "\n",
    ">- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Gathering Research Articles using the OpenAlex API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists\n",
    "\n",
    "### Constructing the Computational Social Scientists Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.1: Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the papers dataset\n",
    "papers_df = pd.read_csv(\"../../comsocsci2024/lectures/papers.csv\")\n",
    "# Load the author's dataset\n",
    "authors_df = pd.read_csv(\"../../comsocsci2024/lectures/authors.csv\")\n",
    "# Load the abstracts dataset\n",
    "abstracts_df = pd.read_csv(\"../../comsocsci2024/lectures/abstracts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   1%|          | 249/37523 [00:58<2:25:55,  4.26it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m papers_authors_ids \u001b[38;5;241m=\u001b[39m papers_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthor_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(ast\u001b[38;5;241m.\u001b[39mliteral_eval)\n\u001b[0;32m     19\u001b[0m author_ids \u001b[38;5;241m=\u001b[39m authors_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcreate_weighted_edgelist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpapers_authors_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthor_ids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#for index, row in papers_df.iterrows():\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m, in \u001b[0;36mcreate_weighted_edgelist\u001b[1;34m(paper_authors_ids, author_ids)\u001b[0m\n\u001b[0;32m     12\u001b[0m                 pairs\u001b[38;5;241m.\u001b[39mappend(pair)\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m authors_ \u001b[38;5;129;01min\u001b[39;00m paper_authors_ids:\n\u001b[1;32m---> 15\u001b[0m                 edges\u001b[38;5;241m.\u001b[39mappend((pair, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m edges\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create weighted edgelist WIP\n",
    "def create_weighted_edgelist(paper_authors_ids, author_ids):\n",
    "    edges = []\n",
    "    pairs = []\n",
    "    # Loop through the paper authors\n",
    "    for authors in tqdm(paper_authors_ids, desc=\"Progress\"):\n",
    "        for i in range(len(authors)):\n",
    "            for j in range(i + 1, len(authors)):\n",
    "                pair = (authors[i], authors[j])\n",
    "                if pair not in pairs and (pair[1], pair[0]) not in pairs:\n",
    "                    co_authored_works_count = 0\n",
    "                    pairs.append(pair)\n",
    "                    for authors_ in paper_authors_ids:\n",
    "                        if pair[0] in authors_ and pair[1] in authors_:\n",
    "                            co_authored_works_count += 1\n",
    "                    edges.append((pair[0], pair[1], co_authored_works_count))\n",
    "    \n",
    "    return edges\n",
    "\n",
    "papers_authors_ids = papers_df[\"author_ids\"].apply(ast.literal_eval)\n",
    "author_ids = authors_df[\"id\"]\n",
    "print(create_weighted_edgelist(papers_authors_ids, author_ids))\n",
    "#for index, row in papers_df.iterrows():\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
