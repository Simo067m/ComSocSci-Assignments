{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02467 Computational Social Science\n",
    "## Assignment 1\n",
    "### Group 15\n",
    "\n",
    "Our GitHub repo is availabe at: https://github.com/Simo067m/ComSocSci-Assignments <br>\n",
    "Contribution:\n",
    "- s233304 : Part 2 + Part 3\n",
    "- s214592 : xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import netwulf\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web-scraping\n",
    "Web-scraping the list of participants to the International Conference in Computational Social Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for finding all unique researchers\n",
    "def scrape_IC2S2(soup : BeautifulSoup):\n",
    "    # Find all the names from the top table\n",
    "    names = []\n",
    "    # Find all the table rows\n",
    "    table_rows = soup.find_all(\"tr\")\n",
    "    for tr in table_rows:\n",
    "        tds = tr.find_all(\"td\")\n",
    "        for row in tds:\n",
    "            a = row.find_all(\"a\")\n",
    "            for text in a:\n",
    "                text_content = text.text\n",
    "                if (\"Keynote\" in text_content):\n",
    "                    text_split = text_content.split(\"-\")\n",
    "                    stripped = text_split[1].strip()\n",
    "                    if (stripped not in names):\n",
    "                        names.append(stripped)\n",
    "    \n",
    "    # Find all the names from the bottom lists\n",
    "    # Find all the unordered lists\n",
    "    ul = soup.find_all(\"ul\", class_=\"nav_list\")\n",
    "    # Find all the list elements\n",
    "    for list in ul:\n",
    "        found_names = list.find_all(\"i\")\n",
    "        # For every found name line, seperate into individual names\n",
    "        for name in found_names:\n",
    "            found_names_seperated = name.text.split(\", \")\n",
    "            for seperated_name in found_names_seperated:\n",
    "                if (seperated_name.strip() not in names):\n",
    "                    names.append(seperated_name.strip())\n",
    "\n",
    "    # Find all the names of the chairs\n",
    "    headers = soup.find_all(\"h2\")\n",
    "    for header in headers:\n",
    "        text = header.find(\"i\")\n",
    "        if (text is not None):\n",
    "            seperated_name = text.text.split(\": \")\n",
    "            if (seperated_name[1].strip() not in names):\n",
    "                names.append(seperated_name[1].strip())\n",
    "\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define url and collect content\n",
    "LINK = \"https://ic2s2-2023.org/program\"\n",
    "r = requests.get(LINK)\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "# find participant names\n",
    "IC2S2_names = scrape_IC2S2(soup)\n",
    "# Save to a pandas DataFrame\n",
    "IC2S2_names_df = pd.DataFrame(IC2S2_names, columns=[\"name\"])\n",
    "IC2S2_names_df.to_csv(\"IC2S2_names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "_How many unique researchers do you get?_\n",
    ">- 1491"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1491\n"
     ]
    }
   ],
   "source": [
    "print(len(pd.read_csv(\"IC2S2_names.csv\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices **(answer in max 150 words)**.\n",
    "\n",
    ">- By inspecting the webpage, we were able to figure out that names were always contained in an &lt;a&gt; element for displaying the name properly. This means that finding an &lt;a&gt; element within one of the tables containing schedules would guarantee a name. When finding other names, like the ones that have the \"chair\", correctness was ensured by splitting that part from the name, ensuring only the name is retrieved.\n",
    ">- Before adding a new name to the list, there is a check making sure that the name is not already in the found names list before adding it, making sure only unique names are in the list. The names contain no unwanted whitespace by calling the $\\texttt{str.strip()}$ method before adding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ready Made vs Custom Made Data\n",
    "\n",
    "### Q1\n",
    "_What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book (answer in max 150 words)._\n",
    "\n",
    ">- **custom-made data**: The pros are that specific hypotheses can be investigated within a  controlled environment without any external influences. However, this control can also be a disadvantage, as the created clusters were created randomly and might only artificially fit the definition of friend group and the studied social network definitions, and people might behave differently as the people that use are not really their friends.\n",
    ">- **ready-made data**: The advantages are that a big dataset is already available and ecologically valid. However, the researchers do not have any control over how the data is collected and this collection purpose might not match the needs of the research. Nevertheless, this can also be seen as a potential pro as this could lead to more creative and innovative methods used by the researchers which might in the next step create new findings outside of the standard procedure.\n",
    "(146 words)\n",
    "\n",
    "### Q2\n",
    "_How do you think these differences can influence the interpretation of the results in each study? (answer in max 150 words)_\n",
    "> These differences can mainly influence the limitations of a study. In Centola's study, the degree of realism within the experimental setup can influence the interpretation. While these results were observed within an artificially created network, it has to be investigated if these outcomes can be generalized to other naturally formed networks. In Nicolaide's study, other effects other than the contagiousness of the network could be the reason for the change in running behaviour. While there were some tests applied to check the outcome, for example, the weather check, it is still always questionable if the \"right\" testing has been done. (100 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Gathering Research Articles using the OpenAlex API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://api.openalex.org\"\n",
    "WORKS_URL = \"/works\"\n",
    "CONCEPT_URL = \"/concepts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieve concepts ids and return tupel of lists that reflect ([Sociology, Psychology, Economics, Political Science], [Mathematics, Physics, Computer science])\n",
    "\"\"\"\n",
    "\n",
    "def create_concept_filters():\n",
    "    #Retrieve concepts so they can be used for filtering\n",
    "    response_concepts = requests.get(BASE_URL + CONCEPT_URL)\n",
    "    all_concepts = (response_concepts.json()['results'])\n",
    "    \n",
    "    # We want either one of: Sociology OR Psychology OR Economics OR Political Science\n",
    "    com_soc_sci_list = ['Sociology', 'Psychology', 'Economics', 'Political science']\n",
    "    # AND intersecting with a quantitative discipline (Mathematics OR Physics OR Computer Science\"\n",
    "    quant_disc_list = ['Mathematics', 'Physics', 'Computer science']\n",
    "           \n",
    "    com_soc_sci_ids = [c['id'] for c in all_concepts if c['display_name'] in com_soc_sci_list]\n",
    "    quant_disc_ids = [c['id'] for c in all_concepts if c['display_name'] in quant_disc_list]\n",
    "\n",
    "    return(com_soc_sci_ids, quant_disc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a parallel executable \"to-do\" list of parameters for retrieving batches of works at once.\n",
    "\"\"\"\n",
    "\n",
    "def create_todo_list(author_id_list):\n",
    "    todo_list = []\n",
    "    batch_size = 25\n",
    "\n",
    "    com_soc_sci_ids, quant_disc_ids = create_concept_filters()\n",
    "    \n",
    "    for i in range(0, len(author_id_list), batch_size):\n",
    "        author_filter = '|'.join(author_id_list[i:i + batch_size])\n",
    "        filter_string = f\"authorships.author.id:{author_filter},cited_by_count:>10,authors_count:<10,concepts.id:{'|'.join(com_soc_sci_ids)},concepts.id:{'|'.join(quant_disc_ids)}\"\n",
    "\n",
    "        params = {\n",
    "            \"filter\": filter_string,\n",
    "            \"per-page\": 200,\n",
    "            \"cursor\": \"*\"\n",
    "        }\n",
    "        todo_list.append(params)\n",
    "    return todo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieve works for the given parameters using paging.\n",
    "\"\"\"\n",
    "\n",
    "def retrieve_all_works(params):\n",
    "    # Pause for 2 seconds to stay within limits of API calls\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # List to store all works retrieved\n",
    "    all_works = []\n",
    "    \n",
    "    # Make requests until all works are fetched\n",
    "    while True:\n",
    "        # Make the GET request\n",
    "        response = requests.get(BASE_URL + WORKS_URL, params=params)\n",
    "        \n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Append retrieved works to the list\n",
    "            all_works.extend(response.json()[\"results\"])\n",
    "            \n",
    "            # Check if there are more works to fetch (paging)\n",
    "            if len(response.json()[\"results\"]) == params[\"per-page\"]:\n",
    "                # Update the cursor for the next page\n",
    "                params[\"cursor\"] = response.json()[\"meta\"][\"next_cursor\"]\n",
    "            else:\n",
    "                # No more works to fetch\n",
    "                break\n",
    "        else:\n",
    "            # Print an error message if the request was not successful\n",
    "            print(\"Error:\", response.status_code, params)\n",
    "            break\n",
    "    \n",
    "    return all_works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Return object including id, publication_year, cited_by_count and author_ids.\n",
    "\"\"\"\n",
    "\n",
    "def create_paper(work):\n",
    "    author_ids = [a['author']['id'] for a in work['authorships']] \n",
    "    return {\n",
    "        'id': work['id'],\n",
    "        'publication_year': work['publication_year'],\n",
    "        'cited_by_count': work['cited_by_count'],\n",
    "        'author_ids': author_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Return object including id, title and abstract_inverted_index.\n",
    "\"\"\"\n",
    "\n",
    "def create_abstract(work):\n",
    "    return {\n",
    "        'id': work['id'],\n",
    "        'title': work['title'],\n",
    "        'abstract_inverted_index': work['abstract_inverted_index']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Process retrieved data and extract relevant information.\n",
    "\"\"\"\n",
    "def process_data():\n",
    "    for upper_list in unprocessed_data:\n",
    "        for work in upper_list:\n",
    "            IC2S2_papers.append(create_paper(work))\n",
    "            IC2S2_abstracts.append(create_abstract(work))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IC2S2_papers = []\n",
    "IC2S2_abstracts = []\n",
    "\n",
    "papers_df = pd.DataFrame()\n",
    "abstracts_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "IC2S2_authors = pd.read_csv('week2-authors.csv')\n",
    "\n",
    "# Filters: Only include IC2S2 authors with a total work count between 5 and 5,000\n",
    "filtered_IC2S2_authors = IC2S2_authors[(IC2S2_authors['works_count'] > 5) & (IC2S2_authors['works_count'] < 5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 48/48 [00:28<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Prepare all request parameters that have to be done in parallel\n",
    "    todo_list = create_todo_list(filtered_IC2S2_authors.id.tolist())\n",
    "    \n",
    "    # Perform all requests with the prepared parameters in parallel\n",
    "    with Pool() as p:\n",
    "        unprocessed_data = list(tqdm(p.imap(retrieve_all_works, todo_list), total=len(todo_list)))\n",
    "\n",
    "    # Process collected data\n",
    "    process_data()   \n",
    "\n",
    "    # Remove duplicates and store in dataframes and \n",
    "    papers_df = pd.DataFrame(IC2S2_papers).drop_duplicates(subset=['id'])\n",
    "    abstracts_df = pd.DataFrame(IC2S2_abstracts).drop_duplicates(subset=['id'])\n",
    "    \n",
    "    # Export data\n",
    "    papers_df.to_csv('papers.csv', index=False)\n",
    "    abstracts_df.to_csv('abstracts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_dict = {'id': papers_df['author_ids'].explode().tolist()}\n",
    "authors_df = pd.DataFrame(data).drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 - Dataset summary\n",
    "- _How many works are listed in your IC2S2 papers dataframe?_\n",
    ">- 10.155\n",
    "- _How many unique researchers have co-authored these works?_\n",
    ">- 15.758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many works are listed in your IC2S2 papers dataframe? 10155\n"
     ]
    }
   ],
   "source": [
    "Q1_works = papers_df.shape[0]\n",
    "print(f\"How many works are listed in your IC2S2 papers dataframe? {Q1_works}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many unique researchers have co-authored these works? 15758\n"
     ]
    }
   ],
   "source": [
    "Q1_authors = authors_df.shape[0]\n",
    "print(f\"How many unique researchers have co-authored these works? {Q1_authors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Q2 - Efficiency in code\n",
    "_Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time?_\n",
    "\n",
    "> I have used multiprocessing of batches with paging while using filters when retrieving the data. \n",
    "First of all, I have filtered the authors from week 2 exercise 2 to start with a smaller set already. I have then created a list of requests that can be executed in parallel. Each of these requests includes the filters (described in the assignment) of the individual request, and the batch size of 200.\n",
    "Next, I executed these requests in parallel using the function “imap” by the class “multiprocessing.Pool”. To keep within the limits of the API I have added a sleep for two seconds. \n",
    "The data retrieval has taken a total of about 36 seconds. (112 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 - Filtering Criteria and Dataset Relevance\n",
    "_Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices?_\n",
    "\n",
    "> When focusing on the total number of works by an author and the citation count, one can say, that while this filtering helps include work from established researchers with a proven track record of impactful research, it excludes upcoming researchers or those from underrepresented backgrounds or smaller fields of computational social science which might be as valuable as the others.\n",
    "This could lead to an overrepresentation of researchers in a more general field and an underrepresentation of niche researchers.\n",
    "When looking into the number of authors per work, can help to create a balanced representation of single-authored and multi-authored works. However, collaborative research projects which may be in interdisciplinary environments will be excluded.\n",
    "When looking into the specific fields, research that does not directly fall into one of these pre-defined fields might still hold a lot of value within these fields and may bridge traditional disciplinary boundaries. (147 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists\n",
    "\n",
    "### Constructing the Computational Social Scientists Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.1: Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the papers dataset\n",
    "#papers_df = pd.read_csv(\"papers.csv\")\n",
    "# Load the author's dataset\n",
    "#authors_df = pd.read_csv(\"week2-authors.csv\")\n",
    "# Load the abstracts dataset\n",
    "#abstracts_df = pd.read_csv(\"abstracts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "malformed node or string: ['https://openalex.org/A5073875709', 'https://openalex.org/A5046943453', 'https://openalex.org/A5034372799', 'https://openalex.org/A5043749084']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m                     edges\u001b[38;5;241m.\u001b[39mappend((pair, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m edges\n\u001b[0;32m---> 16\u001b[0m papers_authors_ids \u001b[38;5;241m=\u001b[39m \u001b[43mpapers_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauthor_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mliteral_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m author_ids \u001b[38;5;241m=\u001b[39m authors_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(create_weighted_edgelist(papers_authors_ids, author_ids))\n",
      "File \u001b[0;32m~/anaconda3/envs/comsocsci2024/lib/python3.10/site-packages/pandas/core/series.py:4904\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4780\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4902\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/comsocsci2024/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/comsocsci2024/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/comsocsci2024/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/comsocsci2024/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/comsocsci2024/lib/python3.10/ast.py:110\u001b[0m, in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/comsocsci2024/lib/python3.10/ast.py:109\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_signed_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/comsocsci2024/lib/python3.10/ast.py:83\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_signed_num\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m operand\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/comsocsci2024/lib/python3.10/ast.py:74\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_num\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_num\u001b[39m(node):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m):\n\u001b[0;32m---> 74\u001b[0m         \u001b[43m_raise_malformed_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/envs/comsocsci2024/lib/python3.10/ast.py:71\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._raise_malformed_node\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lno \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(node, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlineno\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     70\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: malformed node or string: ['https://openalex.org/A5073875709', 'https://openalex.org/A5046943453', 'https://openalex.org/A5034372799', 'https://openalex.org/A5043749084']"
     ]
    }
   ],
   "source": [
    "# Create weighted edgelist WIP\n",
    "def create_weighted_edgelist(paper_authors_ids, author_ids):\n",
    "    edges = []\n",
    "    # Loop through the paper authors\n",
    "    for authors in tqdm(paper_authors_ids, desc=\"Progress\"):\n",
    "        pairs = []\n",
    "        for i in range(len(authors)):\n",
    "            for j in range(i + 1, len(authors)):\n",
    "                pair = (authors[i], authors[j])\n",
    "\n",
    "                if pair not in pairs and (pair[1], pair[0]) not in pairs:\n",
    "                    pairs.append(pair)\n",
    "                    edges.append((pair, 1))\n",
    "    return edges\n",
    "\n",
    "papers_authors_ids = papers_df[\"author_ids\"].apply(ast.literal_eval)\n",
    "author_ids = authors_df[\"id\"]\n",
    "print(create_weighted_edgelist(papers_authors_ids, author_ids))\n",
    "#for index, row in papers_df.iterrows():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
